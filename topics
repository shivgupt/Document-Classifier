#!/usr/bin/python

"""
Mode = Train
The program starts by recursively going through the directory and parse document to represent it as a vector of words.
 We have ignored all the words in the document that occur in stopwords.txt, which consist of most frequently occuring
 words like a, the, is etc.

 Every time we read a new document we decide whether to look at label or not based on the fraction and if label is
 unkown the document is added to the list of unknown variables.

 In case of fraction 1, Model is based on Baysean Network, where
 P(W|T) = number of docs in Topic containing w
        ----------------------------------------
            number of docs in Topic

P(T) = number of docs in Topic
      --------------------------
        total number of docs

In case of of fraction lower than 1.0

For the known documents we have P(T|D) = 1, in case the document belongs to T and 0 otherwise.
But for the unknown documents we iterate for N times, and calculate P(T|D) = a P(T) P(Wi|T).....P(Wn|T) , where a is
normalization constant and W's are all the words in the document.

P(W|T) = number of known docs in Topic containing w + fraction of unkown docs in Topic containing w
        --------------------------------------------------------------------------------
                                    number of docs in Topic

P(T) = number of docs in Topic + fraction of unkown docs in Topic
      -------------------------------------------------------------
                        total number of docs


At the end it outputs top 20 words in each topic

Mode = Test
Reads the model File and Creates the table of P(W|T).

It calculates P(T) P(Wi|T).....P(Wn|T) for all W's i.e. the words that are in the document and
assigns the labels which has highest probability.

If the word is not found in the label i.e. P(W|T) = 0.0001 by default.

Accuraacy       f
79.27%          1
77.94%          0.9
75.72%          0.8
70.52%          0.5
15.00%          0.1

"""


import os
import sys
import time
import numpy
import re
import math
import random

start = time.time()

mode = sys.argv[1]
dataset_path = sys.argv[2]
model_file = sys.argv[3]
f = float(sys.argv[4])
#f = 1

ConfusionMatrix = {}
vocabulary = {}     # <word : global frequency>
labels = {}         # <label : [ total number of documents, <word : number documents containing this word>] >
unknown = []        #  list of unlabeled documents
PT = {}             # <label : <documentID : P(Ti|D) > >
C = {}              # <label : <word : P(Ti|W) > >
A = {}              # <label : sum of fractions unlabeled docs assigned >
total = 0
N = 3

def get_stopwords():
    stopwords = {}
    stop_file = open('stopwords.txt','r')
    for line in stop_file:
        data = line.rstrip('\n')
        stopwords.setdefault(data,0)
    stop_file.close()

    return stopwords
'''
Decides whether to look at the label of document and if label is known increment the word count of that label for all
the words that occur in the document.

Returns document as bag-of-words
'''

def get_bag_of_words(root, file, path,trainset):
    fname = os.path.join(root, file)
    global vocabulary, stopwords, labels, f
    true_label = os.path.relpath(root, path)
    if f < 1:
        if f == 0.0:
            unknown.append(total)
            l = '?'

        if int(numpy.random.choice([0, 1], 1, True, [f, 1 - f])) == 0:
            l = true_label
            labels[l][0] += 1
        else:
            l = '?'
            unknown.append(total)
    else:
        l = true_label
        labels[l][0] += 1

    trainset[total] = [{}, true_label]
    fd = open(fname, 'r')
    for line in fd:
        only_letters = re.sub("[^a-zA-Z]", " ",line)
        for word in only_letters.lower().split():
            if word in stopwords: continue
            try:
                trainset[total][0][word] += 1
            except:
                trainset[total][0][word] = 1
                try:
                    vocabulary[word] += 1
                    if l != '?':
                        try:
                            labels[l][1][word] += 1
                        except:
                            labels[l][1][word] = 1
                    else:
                        labels[l][1][word] = 0
                except:
                    vocabulary[word] = 1
                    if l != '?':
                        labels[l][1][word] = 1
    fd.close()
    return
'''
Recursively Read Documents in all the folders and stores them into trainset as vector of words.

Each documen ID consists of a dictionary, with words as keys and their frequency as values

'''
def learn(path):
    global labels,f, total
    trainset = {}                       # <documentID : [<word : frequency> , true_label]
    for root, dirs, files in os.walk(path):
        for d in dirs:
            labels.setdefault(d,[0,{}])
        for file in files:
            if file[0] == '.': continue
            total +=1
            get_bag_of_words(root, file, path, trainset)
    return trainset

def print_performance(CM,Accuracy):


    print "Accuracy = ", Accuracy*100/(1.0*total)
    print "*******Confusion Matrix*******\n             "

    for l in labels:
        sys.stdout.write(l+"       ")
    print

    for a in ConfusionMatrix:
        side = True
        for p in ConfusionMatrix[a]:
            if side == True:
                sys.stdout.write( a +"      ")
                side = False
            sys.stdout.write( str(ConfusionMatrix[a][p]) + "          ")
        print

def get_label(document):
    temp = {}
    for l in labels:
        temp[l] = math.log(labels[l][0])
        for w in document:
            try:
                temp[l] += math.log(labels[l][1][w])
            except:
                temp[l] += math.log(0.0001)
    v = list(temp.values())
    k = list(temp.keys())
    return k[v.index(max(v))]

def classify(root, file, path):
    fname = os.path.join(root, file)
    global trainset, vocabulary, stopwords, labels
    true_lablel = os.path.relpath(root, path)
    document_vector = {}
    fd = open(fname, 'r')
    for line in fd:
        only_letters = re.sub("[^a-zA-Z]", " ", line)
        for word in only_letters.lower().split():
            if word in stopwords: continue
            try:
                document_vector[word] += 1
            except:
                document_vector[word] = 1
    fd.close()

    return (true_lablel,get_label(document_vector))

def write_distinct_words(file):
    f = open(file, 'w')
    for l in labels:
        f.write("\nMost frequent words in "+l +" are : \n")
        Words = sorted(labels[l][1].keys(),key=labels[l][1].get,reverse=True)
        for w in Words[0:20]:
            f.write(w+" \n")
    f.close()


'''
Writes Model as :
Label1=P(Label1)
Word1    P(Label1|Word1)
.
.
WordN=P(Label1|WordN)
Label1=P(Label1)
Word1    P(Label2|Word1)
.
.
.
WordN=P(LabelN|WordN)
'''

def get_model(file):
    global labels, ConfusionMatrix
    f = open(file, 'r')
    for line in f:
        data = line.rstrip('\n').split()
        if len(data) == 1:
            data = data[0].split('=')
            l = data[0]
            labels[l] = (float(data[1]),{})
        else:
            labels[l][1][data[0]] = float(data[1])
    f.close()
    ConfusionMatrix = {j: {i: 0 for i in labels} for j in labels}
    return

def test(path):
    global total, ConfusionMatrix
    Accuracy = 0
    f = open('result.txt','w')
    for root, dirs, files in os.walk(path):
        for d in dirs:
            labels.setdefault(d,([],{}))
        for file in files:
            if file[0] == '.': continue
            total+=1
            (true_label, predicted_label) = classify(root, file, path)
            f.write(true_label+ ' ' + predicted_label + '\n')
            if true_label == predicted_label:
                Accuracy +=1
                ConfusionMatrix[true_label][true_label] +=1
            else:
                ConfusionMatrix[true_label][predicted_label] += 1

    print_performance(ConfusionMatrix,Accuracy)
    return

def set_model (file):
    with open(file,'w') as f:
        for l in labels:
            f.write(l+'=' + str(labels[l][0]/(1.0*total)) + '\n')
            for w in labels[l][1]:
                if vocabulary[w] > 1:
                    try:
                        labels[l][1][w] = labels[l][1][w]/(1.0 * labels[l][0])
                        f.write(w + ' ' + str(labels[l][1][w]) + '\n')
                    except:
                        pass
                else:
                    try:
                        labels[l][1][w] = -1
                    except:
                        pass
    f.close()
    return

'''
Calculates logP(L|D) = log aP(L)* product(P(Wi|L))

P(Wi|L) = docs containing Wi in L
        ----------------------------
                total docs in L

if none of the words in document exists in L, then P(L|D) is calculated randomly
'''


def get_pld(l,d,A,C,Z):
    global PT
    pwl = 0
    llen = labels[l][0]+A
    pld = math.log(llen/(1.0*total)) - Z
    for w in trainset[d][0]:
        try:
            a = labels[l][1][w]
        except:
            labels[l][1][w] = 1
            a = 1
        try:
            b = C[w]
        except:
            b = 0
            C[w] = 0
        try:
            pwl += math.log((a+b)/llen)
        except:
            pass

    if pwl != 0:
        return pld+pwl

"""
Assign the unlabled document to the most likely label and update the word count of that label

"""
def assign_label(d):
    global PT, labels, temp

    l = '?'
    m = 0
    for p in labels:
        try:
            t = math.exp(PT[p][d])
            if m < t:
                m = t
                l = p
        except:
            pass

    try:
        labels[l][0] += 1
    except:
        l = random.choice(labels.keys())
        labels[l][0] += 1
    for w in trainset[d][0]:
        try:
            labels[l][1][w] += 1
        except:
            labels[l][1][w] = 1

'''
Update the count of documents containing Wi in labels based on the Distribution.
Normalize the probability distribution of the label.

'''

def update_frequency(A,C,Z):
    global PT
    for l in labels:
        A[l] = 0
        for d in unknown:
            try:
                PT[l][d] -= Z[d]
                temp = math.exp(PT[l][d])
            except:
                pass
            try:
                A[l] += temp
            except:
                A[l] = random.uniform(0,len(unknown))
            for w in trainset[d][0]:
                try:
                    C[l][w] += temp
                except:
                    C[l][w] = temp
'''

It assigns all the unlabeled documents to the most probable label based on the Gaussian distribution obtained
for each document.

'''
def get_cluster():
    global PT, labels, C, A
    Z={}        # sumj PT[Lj][D]
    unlabled = len(unknown)
    for i in range(N):
        for d in unknown:
            temp = 0.0
            for l in labels:
                # probability of label given the document
                pld = get_pld(l, d, A.setdefault(l,unlabled/20.0),C.setdefault(l,{}),Z.setdefault(d,0))
                try:
                    PT[l][d] = pld
                except:
                    PT[l] = {}
                    PT[l][d] = pld
                try:
                    temp += math.exp(PT[l][d])
                except:
                    pass
            try:
                Z[d] = math.log(temp)
            except:
                Z[d] = 0
        update_frequency(A,C,Z)

    for d in unknown:
        assign_label(d)

stopwords = get_stopwords()     # <word : 0>,  irrelevant words


'''
create a model base on -

1. Bayesian Network
2. Bayesian Network with cross-validation done using Gaussian Mixture, where fraction of dataset is kept for tuning -
labels are not seen

'''
if mode == 'train':
    trainset = learn(dataset_path)
    if f < 1.0:
        print len(unknown)
        get_cluster()
    set_model(model_file)
    write_distinct_words('distinct.txt')
else:
    get_model(model_file)
    test(dataset_path)

print "total time = ", time.time() - start